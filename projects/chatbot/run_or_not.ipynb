{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c34ca065-8426-4162-8184-f47fcfb4ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8776451d-90f3-44d5-a1af-af43d0ca30ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osungar\\AppData\\Local\\Temp\\ipykernel_21672\\470010239.py:6: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  paper = next(search.results())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models for Software Engineering: A Systematic Literature Review\n",
      "A Comprehensive Overview of Large Language Models\n",
      "A Survey of Large Language Models\n",
      "Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers\n",
      "Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations\n",
      "Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "# papers about LLMs\n",
    "paper_ids = ['2308.10620', '2307.06435', '2303.18223', '2307.10700', '2310.11207', '2305.11828']\n",
    "for paper_id in paper_ids:\n",
    "    search = arxiv.Search(id_list=[paper_id])\n",
    "    paper = next(search.results())\n",
    "    print(paper.title)\n",
    "    \n",
    "    paper.download_pdf('C:\\\\Users\\\\osungar\\\\Desktop\\\\projects\\\\chatbot\\\\data\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fe72294-a669-4541-b892-6fa75757d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b02e5c1-def9-4b90-bd0b-87d92b241521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define what documents to load\n",
    "loader = DirectoryLoader(path='C:\\\\Users\\\\osungar\\\\Desktop\\\\projects\\\\chatbot\\\\data\\\\', glob=\"*.pdf\", loader_cls=PyPDFLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99ae6982-9bdb-45aa-9ee9-317beda4942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret information in the documents\n",
    "documents = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                          chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b087035-5611-4b8c-8624-e1d9f3692030",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0581ff14-cb72-4025-a002-823fb88536aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "024b40c1-d097-490e-9b30-dd197c8ead15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vector store database\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4881882a-c69c-4a6a-b8d3-2b360be57695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the vector store\n",
    "db.save_local(\"C:\\\\Users\\\\osungar\\\\Desktop\\\\projects\\\\chatbot\\\\data\\\\faiss\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f351d450-4d08-49e7-88ff-acfebcca7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer modeli C dilinde yapılmış\n",
    "from langchain.llms import CTransformers\n",
    "def load_llm(): \n",
    "  #load the lm\n",
    "  llm = CTransformers(model='C:\\\\Users\\\\osungar\\\\Desktop\\\\projects\\\\chatbot\\\\data\\\\models\\\\llama-2–7b-chat.ggmlv3.q2_K.bin',\n",
    "  # model available here: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main\n",
    "  model_type='llama',\n",
    "  config={'max_new_tokens': 256, 'temperature': 0})\n",
    "  return llm   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5103ca60-b599-4a68-b9a6-17facbea2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_store(): \n",
    "  # load the vector store\n",
    "  embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                model_kwargs={'device': 'cpu'})\n",
    "  db = FAISS.load_local('faiss', embeddings)\n",
    "  return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dab20f55-bf4f-4c4c-85fe-c3cb23557bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt eklenmesi\n",
    "from langchain import PromptTemplate\n",
    "def create_prompt_template():\n",
    "  # prepare the template that provides instructions to the chatbot\n",
    "  template = \"\"\"Use the provided context to answer the user’s question.\n",
    "                If you don’t know the answer, respond with \"I do not know\".\n",
    "                Context: {context}\n",
    "                History: {history}\n",
    "                Question: {question}\n",
    "                Answer: \"\"\"\n",
    "  prompt = PromptTemplate(\n",
    "                template=template,\n",
    "                input_variables=['context', 'question'])\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "caea37bc-4f4e-4020-ac14-559c7a413635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#soru cevap zinciri\n",
    "from langchain.chains import RetrievalQA\n",
    "def create_qa_chain():\n",
    "  \"\"\"create the qa chain\"\"\"\n",
    "  # load the llm, vector store, and the prompt\n",
    "  llm = load_llm()\n",
    "  db = load_vector_store()\n",
    "  prompt = create_prompt_template()\n",
    "  # create the qa_chain\n",
    "  retriever = db.as_retriever(search_kwargs={'k': 2})\n",
    "  qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                            chain_type='stuff',\n",
    "                            retriever=retriever,\n",
    "                            return_source_documents=True,\n",
    "                            chain_type_kwargs={'prompt': prompt})\n",
    "  return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a03f81b-b129-4b0f-a3a8-4144223d10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cevap döndürecek fonksiyon\n",
    "def generate_response(query, qa_chain):\n",
    "  # use the qa_chain to answer the given query\n",
    "  return qa_chain({'query':query})['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8148eb-f8fb-4e3b-be03-28e492fe1f76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
